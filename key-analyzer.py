#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPT-Load å¯†é‘°åˆ†æå·¥å…·
åˆ†ææ¸¬è©¦çµæœä¸¦ç”Ÿæˆè©³ç´°å ±å‘Š
"""

import pandas as pd
import json
import argparse
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

class KeyAnalyzer:
    def __init__(self, csv_file: str):
        self.df = pd.read_csv(csv_file)
        self.total_keys = len(self.df)
        
    def basic_stats(self):
        """åŸºæœ¬çµ±è¨ˆè³‡è¨Š"""
        valid_keys = self.df[self.df['valid'] == True]
        invalid_keys = self.df[self.df['valid'] == False]
        
        stats = {
            'total_keys': self.total_keys,
            'valid_keys': len(valid_keys),
            'invalid_keys': len(invalid_keys),
            'valid_percentage': len(valid_keys) / self.total_keys * 100,
            'avg_response_time': self.df[self.df['response_time_ms'] > 0]['response_time_ms'].mean(),
            'median_response_time': self.df[self.df['response_time_ms'] > 0]['response_time_ms'].median(),
        }
        
        return stats
    
    def error_analysis(self):
        """éŒ¯èª¤åˆ†æ"""
        invalid_keys = self.df[self.df['valid'] == False]
        error_counts = Counter(invalid_keys['error_message'])
        
        return dict(error_counts.most_common())
    
    def group_analysis(self):
        """åˆ†çµ„åˆ†æ"""
        group_stats = self.df.groupby('group_id').agg({
            'valid': ['count', 'sum'],
            'response_time_ms': 'mean'
        }).round(2)
        
        group_stats.columns = ['total_keys', 'valid_keys', 'avg_response_time']
        group_stats['valid_percentage'] = (group_stats['valid_keys'] / group_stats['total_keys'] * 100).round(2)
        
        return group_stats
    
    def response_time_analysis(self):
        """å›æ‡‰æ™‚é–“åˆ†æ"""
        valid_keys = self.df[(self.df['valid'] == True) & (self.df['response_time_ms'] > 0)]
        
        if len(valid_keys) == 0:
            return {}
        
        return {
            'min_response_time': valid_keys['response_time_ms'].min(),
            'max_response_time': valid_keys['response_time_ms'].max(),
            'avg_response_time': valid_keys['response_time_ms'].mean(),
            'median_response_time': valid_keys['response_time_ms'].median(),
            'p95_response_time': valid_keys['response_time_ms'].quantile(0.95),
            'p99_response_time': valid_keys['response_time_ms'].quantile(0.99),
        }
    
    def generate_report(self, output_file: str = None):
        """ç”Ÿæˆè©³ç´°å ±å‘Š"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"key_analysis_report_{timestamp}.txt"
        
        stats = self.basic_stats()
        errors = self.error_analysis()
        groups = self.group_analysis()
        response_times = self.response_time_analysis()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("GPT-Load å¯†é‘°æ¸¬è©¦åˆ†æå ±å‘Š\n")
            f.write("=" * 60 + "\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # åŸºæœ¬çµ±è¨ˆ
            f.write("ğŸ“Š åŸºæœ¬çµ±è¨ˆ\n")
            f.write("-" * 30 + "\n")
            f.write(f"ç¸½å¯†é‘°æ•¸é‡: {stats['total_keys']:,}\n")
            f.write(f"æœ‰æ•ˆå¯†é‘°æ•¸é‡: {stats['valid_keys']:,}\n")
            f.write(f"ç„¡æ•ˆå¯†é‘°æ•¸é‡: {stats['invalid_keys']:,}\n")
            f.write(f"æœ‰æ•ˆç‡: {stats['valid_percentage']:.2f}%\n")
            f.write(f"å¹³å‡å›æ‡‰æ™‚é–“: {stats['avg_response_time']:.2f} ms\n")
            f.write(f"ä¸­ä½æ•¸å›æ‡‰æ™‚é–“: {stats['median_response_time']:.2f} ms\n\n")
            
            # éŒ¯èª¤åˆ†æ
            f.write("âŒ éŒ¯èª¤åˆ†æ\n")
            f.write("-" * 30 + "\n")
            for error, count in errors.items():
                percentage = count / stats['invalid_keys'] * 100 if stats['invalid_keys'] > 0 else 0
                f.write(f"{error}: {count} ({percentage:.1f}%)\n")
            f.write("\n")
            
            # åˆ†çµ„åˆ†æ
            f.write("ğŸ“ åˆ†çµ„åˆ†æ\n")
            f.write("-" * 30 + "\n")
            f.write(groups.to_string())
            f.write("\n\n")
            
            # å›æ‡‰æ™‚é–“åˆ†æ
            if response_times:
                f.write("â±ï¸ å›æ‡‰æ™‚é–“åˆ†æ (åƒ…æœ‰æ•ˆå¯†é‘°)\n")
                f.write("-" * 30 + "\n")
                f.write(f"æœ€å°å›æ‡‰æ™‚é–“: {response_times['min_response_time']:.2f} ms\n")
                f.write(f"æœ€å¤§å›æ‡‰æ™‚é–“: {response_times['max_response_time']:.2f} ms\n")
                f.write(f"å¹³å‡å›æ‡‰æ™‚é–“: {response_times['avg_response_time']:.2f} ms\n")
                f.write(f"ä¸­ä½æ•¸å›æ‡‰æ™‚é–“: {response_times['median_response_time']:.2f} ms\n")
                f.write(f"95% åˆ†ä½æ•¸: {response_times['p95_response_time']:.2f} ms\n")
                f.write(f"99% åˆ†ä½æ•¸: {response_times['p99_response_time']:.2f} ms\n\n")
        
        print(f"è©³ç´°å ±å‘Šå·²å„²å­˜åˆ°: {output_file}")
        return output_file
    
    def export_valid_keys(self, output_file: str = None):
        """åŒ¯å‡ºæœ‰æ•ˆå¯†é‘°æ¸…å–®"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"valid_keys_{timestamp}.csv"
        
        valid_keys = self.df[self.df['valid'] == True]
        valid_keys.to_csv(output_file, index=False, encoding='utf-8')
        
        print(f"æœ‰æ•ˆå¯†é‘°æ¸…å–®å·²åŒ¯å‡ºåˆ°: {output_file}")
        return output_file
    
    def export_invalid_keys(self, output_file: str = None):
        """åŒ¯å‡ºç„¡æ•ˆå¯†é‘°æ¸…å–®"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"invalid_keys_{timestamp}.csv"
        
        invalid_keys = self.df[self.df['valid'] == False]
        invalid_keys.to_csv(output_file, index=False, encoding='utf-8')
        
        print(f"ç„¡æ•ˆå¯†é‘°æ¸…å–®å·²åŒ¯å‡ºåˆ°: {output_file}")
        return output_file

def main():
    parser = argparse.ArgumentParser(description='GPT-Load å¯†é‘°åˆ†æå·¥å…·')
    parser.add_argument('csv_file', help='æ¸¬è©¦çµæœ CSV æª”æ¡ˆ')
    parser.add_argument('--report', action='store_true', help='ç”Ÿæˆè©³ç´°å ±å‘Š')
    parser.add_argument('--valid-keys', action='store_true', help='åŒ¯å‡ºæœ‰æ•ˆå¯†é‘°')
    parser.add_argument('--invalid-keys', action='store_true', help='åŒ¯å‡ºç„¡æ•ˆå¯†é‘°')
    
    args = parser.parse_args()
    
    try:
        analyzer = KeyAnalyzer(args.csv_file)
        
        # é¡¯ç¤ºåŸºæœ¬çµ±è¨ˆ
        stats = analyzer.basic_stats()
        print("=" * 50)
        print("åŸºæœ¬çµ±è¨ˆè³‡è¨Š")
        print("=" * 50)
        print(f"ç¸½å¯†é‘°æ•¸é‡: {stats['total_keys']:,}")
        print(f"æœ‰æ•ˆå¯†é‘°æ•¸é‡: {stats['valid_keys']:,}")
        print(f"ç„¡æ•ˆå¯†é‘°æ•¸é‡: {stats['invalid_keys']:,}")
        print(f"æœ‰æ•ˆç‡: {stats['valid_percentage']:.2f}%")
        print(f"å¹³å‡å›æ‡‰æ™‚é–“: {stats['avg_response_time']:.2f} ms")
        
        if args.report:
            analyzer.generate_report()
        
        if args.valid_keys:
            analyzer.export_valid_keys()
        
        if args.invalid_keys:
            analyzer.export_invalid_keys()
            
    except Exception as e:
        print(f"éŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
